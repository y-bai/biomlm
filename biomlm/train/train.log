WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
04/11/2024 22:07:59 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
04/11/2024 22:07:59 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
04/11/2024 22:07:59 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
04/11/2024 22:07:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
04/11/2024 22:07:59 - INFO - __main__ - Start generate dataset for T2T
04/11/2024 22:07:59 - INFO - __main__ - use_streaming: True
04/11/2024 22:07:59 - INFO - __main__ - chunk length for original sequence: 20000
04/11/2024 22:07:59 - INFO - __main__ - overlap: 0
Using custom data configuration 20kbp-74621d2be131d248
04/11/2024 22:07:59 - INFO - datasets.builder - Using custom data configuration 20kbp-74621d2be131d248
Loading Dataset Infos from /home/share/huadjyin/home/baiyong01/.cache/huggingface/modules/datasets_modules/datasets/_dataset_t2t/851915b39fe3acb88d63ef80d3069d4478b6dbad1995deb60fa19660a048c0af
04/11/2024 22:07:59 - INFO - datasets.info - Loading Dataset Infos from /home/share/huadjyin/home/baiyong01/.cache/huggingface/modules/datasets_modules/datasets/_dataset_t2t/851915b39fe3acb88d63ef80d3069d4478b6dbad1995deb60fa19660a048c0af
/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.
04/11/2024 22:08:00 - WARNING - datasets.builder - Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:22, 22.55s/ examples]Generating train split: 1098 examples [00:22, 68.99 examples/s]Generating train split: 3000 examples [00:22, 237.23 examples/s]Generating train split: 5000 examples [00:22, 483.57 examples/s]Generating train split: 7000 examples [00:23, 822.75 examples/s]Generating train split: 9000 examples [00:23, 1281.22 examples/s]Generating train split: 11000 examples [00:23, 1888.81 examples/s]Generating train split: 12000 examples [00:39, 1888.81 examples/s]Generating train split: 12420 examples [00:45, 223.55 examples/s] Generating train split: 13197 examples [00:45, 267.51 examples/s]Generating train split: 15000 examples [00:45, 414.30 examples/s]Generating train split: 17000 examples [00:45, 632.30 examples/s]Generating train split: 19000 examples [00:46, 941.33 examples/s]Generating train split: 21000 examples [00:46, 1360.13 examples/s]Generating train split: 23000 examples [00:46, 1920.74 examples/s]Generating train split: 24000 examples [00:59, 1920.74 examples/s]Generating train split: 24554 examples [01:04, 287.20 examples/s] Generating train split: 25001 examples [01:04, 315.27 examples/s]Generating train split: 27000 examples [01:04, 505.43 examples/s]Generating train split: 29000 examples [01:05, 763.09 examples/s]Generating train split: 31000 examples [01:05, 1124.60 examples/s]Generating train split: 33000 examples [01:05, 1614.21 examples/s]Generating train split: 34000 examples [01:19, 1614.21 examples/s]Generating train split: 34609 examples [01:22, 292.55 examples/s] Generating train split: 35441 examples [01:23, 348.46 examples/s]Generating train split: 37089 examples [01:23, 505.55 examples/s]Generating train split: 39000 examples [01:23, 762.57 examples/s]Generating train split: 41000 examples [01:23, 1134.22 examples/s]Generating train split: 43000 examples [01:23, 1637.65 examples/s]Generating train split: 44000 examples [01:40, 1637.65 examples/s]Generating train split: 44287 examples [01:40, 289.26 examples/s] Generating train split: 45256 examples [01:40, 357.77 examples/s]Generating train split: 47153 examples [01:40, 551.54 examples/s]Generating train split: 49000 examples [01:40, 813.39 examples/s]Generating train split: 51279 examples [01:40, 1248.83 examples/s]Generating train split: 53000 examples [01:40, 1696.37 examples/s]Generating train split: 55118 examples [01:56, 346.64 examples/s] Generating train split: 57000 examples [01:56, 489.37 examples/s]Generating train split: 59000 examples [01:56, 702.17 examples/s]Generating train split: 61000 examples [01:56, 997.25 examples/s]Generating train split: 61000 examples [02:11, 997.25 examples/s]Generating train split: 61995 examples [02:11, 290.73 examples/s]Generating train split: 63039 examples [02:11, 365.62 examples/s]Generating train split: 65000 examples [02:11, 564.40 examples/s]Generating train split: 67000 examples [02:11, 848.82 examples/s]Generating train split: 69000 examples [02:11, 1234.71 examples/s]Generating train split: 71000 examples [02:25, 365.85 examples/s] Generating train split: 72352 examples [02:25, 475.02 examples/s]Generating train split: 74001 examples [02:25, 662.90 examples/s]Generating train split: 76000 examples [02:25, 971.94 examples/s]Generating train split: 77764 examples [02:39, 328.79 examples/s]Generating train split: 79198 examples [02:39, 438.20 examples/s]Generating train split: 81098 examples [02:39, 643.89 examples/s]Generating train split: 83000 examples [02:39, 929.20 examples/s]Generating train split: 84000 examples [02:52, 929.20 examples/s]Generating train split: 84865 examples [02:52, 359.85 examples/s]Generating train split: 86000 examples [02:52, 451.81 examples/s]Generating train split: 87442 examples [02:52, 617.66 examples/s]Generating train split: 89171 examples [02:52, 895.95 examples/s]Generating train split: 91000 examples [02:52, 1300.43 examples/s]Generating train split: 92778 examples [03:05, 369.65 examples/s] Generating train split: 94124 examples [03:05, 489.08 examples/s]Generating train split: 96000 examples [03:05, 724.38 examples/s]Generating train split: 98000 examples [03:05, 1071.87 examples/s]Generating train split: 100001 examples [03:17, 382.44 examples/s]Generating train split: 102000 examples [03:17, 554.47 examples/s]Generating train split: 104000 examples [03:17, 794.95 examples/s]Generating train split: 106472 examples [03:28, 433.47 examples/s]Generating train split: 108173 examples [03:28, 579.64 examples/s]Generating train split: 110000 examples [03:28, 799.14 examples/s]Generating train split: 112063 examples [03:37, 441.22 examples/s]Generating train split: 114000 examples [03:37, 620.68 examples/s]Generating train split: 115760 examples [03:46, 385.35 examples/s]Generating train split: 117000 examples [03:46, 488.37 examples/s]Generating train split: 118646 examples [03:47, 682.43 examples/s]Generating train split: 120307 examples [03:47, 954.16 examples/s]Generating train split: 121849 examples [03:55, 424.03 examples/s]Generating train split: 123256 examples [03:56, 572.63 examples/s]Generating train split: 125001 examples [03:56, 827.79 examples/s]Generating train split: 126834 examples [04:03, 457.88 examples/s]Generating train split: 128598 examples [04:03, 655.71 examples/s]Generating train split: 129776 examples [04:11, 385.98 examples/s]Generating train split: 131000 examples [04:11, 511.09 examples/s]Generating train split: 132862 examples [04:11, 779.67 examples/s]Generating train split: 134079 examples [04:17, 472.82 examples/s]Generating train split: 135980 examples [04:17, 724.75 examples/s]Generating train split: 137219 examples [04:23, 444.78 examples/s]Generating train split: 139000 examples [04:23, 662.55 examples/s]Generating train split: 140198 examples [04:37, 242.98 examples/s]Generating train split: 141220 examples [04:37, 313.47 examples/s]Generating train split: 142380 examples [04:38, 426.25 examples/s]Generating train split: 144010 examples [04:38, 649.65 examples/s]Generating train split: 146000 examples [04:38, 1026.81 examples/s]Generating train split: 147909 examples [04:38, 1359.69 examples/s]Generating train split: 147909 examples [04:38, 530.40 examples/s] 
Setting num_proc from 4 back to 1 for the validation split to disable multiprocessing as it only contains one shard.
04/11/2024 22:12:39 - WARNING - datasets.builder - Setting num_proc from 4 back to 1 for the validation split to disable multiprocessing as it only contains one shard.
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 1 examples [00:21, 21.04s/ examples]Generating validation split: 1687 examples [00:21, 113.74 examples/s]Generating validation split: 2254 examples [00:22, 98.25 examples/s] 
Setting num_proc from 4 back to 1 for the test split to disable multiprocessing as it only contains one shard.
04/11/2024 22:13:02 - WARNING - datasets.builder - Setting num_proc from 4 back to 1 for the test split to disable multiprocessing as it only contains one shard.
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 1 examples [00:21, 21.59s/ examples]Generating test split: 1373 examples [00:21, 90.25 examples/s]Generating test split: 2566 examples [00:23, 111.21 examples/s]
Found cached dataset _dataset_t2t (/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/datasets/T2T_BPE_50010_1024/_dataset_t2t/20kbp-74621d2be131d248/1.0.0/851915b39fe3acb88d63ef80d3069d4478b6dbad1995deb60fa19660a048c0af)
04/11/2024 22:13:25 - INFO - datasets.builder - Found cached dataset _dataset_t2t (/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/datasets/T2T_BPE_50010_1024/_dataset_t2t/20kbp-74621d2be131d248/1.0.0/851915b39fe3acb88d63ef80d3069d4478b6dbad1995deb60fa19660a048c0af)
Loading Dataset info from /home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/datasets/T2T_BPE_50010_1024/_dataset_t2t/20kbp-74621d2be131d248/1.0.0/851915b39fe3acb88d63ef80d3069d4478b6dbad1995deb60fa19660a048c0af
04/11/2024 22:13:25 - INFO - datasets.info - Loading Dataset info from /home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/datasets/T2T_BPE_50010_1024/_dataset_t2t/20kbp-74621d2be131d248/1.0.0/851915b39fe3acb88d63ef80d3069d4478b6dbad1995deb60fa19660a048c0af
04/11/2024 22:16:31 - INFO - __main__ - IterableDatasetDict({
    train: IterableDataset({
        features: ['sequence', 'chromosome', 'start_pos', 'end_pos'],
        n_shards: 147909
    })
    validation: IterableDataset({
        features: ['sequence', 'chromosome', 'start_pos', 'end_pos'],
        n_shards: 2254
    })
    test: IterableDataset({
        features: ['sequence', 'chromosome', 'start_pos', 'end_pos'],
        n_shards: 2566
    })
})
04/11/2024 22:16:31 - INFO - __main__ - Tokenizer model name: BPE
04/11/2024 22:16:31 - INFO - __main__ - Fast tokenizer: True
04/11/2024 22:16:31 - INFO - __main__ - Pretrained tokenizer loading from: /home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/mambaDNA/tokens/T2T/1K/BPE/50010
04/11/2024 22:16:31 - INFO - __main__ - tokenization_config.model_max_length: 1024
04/11/2024 22:16:31 - INFO - __main__ - training_config.sharded_ddp: True
[INFO|tokenization_utils_base.py:2044] 2024-04-11 22:16:31,348 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2044] 2024-04-11 22:16:31,348 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2044] 2024-04-11 22:16:31,348 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2044] 2024-04-11 22:16:31,348 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2044] 2024-04-11 22:16:31,348 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2044] 2024-04-11 22:16:31,348 >> loading file tokenizer_config.json
04/11/2024 22:16:31 - INFO - __main__ - vocab size load by tokenizer: 50010
04/11/2024 22:16:34 - INFO - __main__ - IterableDatasetDict({
    train: IterableDataset({
        features: ['input_ids', 'attention_mask'],
        n_shards: 147909
    })
    validation: IterableDataset({
        features: ['input_ids', 'attention_mask'],
        n_shards: 2254
    })
    test: IterableDataset({
        features: ['input_ids', 'attention_mask'],
        n_shards: 2566
    })
})
length of sample: 1024
total samples: 1482
{'input_ids': [623, 3330, 67, 1916, 326, 1279, 226, 668, 104, 2306, 50, 526, 1243, 83, 1168, 30, 1294, 9375, 1723, 127, 384, 3380, 4932, 30, 3250, 169, 38503, 813, 1804, 125, 650, 2627, 1389, 360, 1483, 14420, 19426, 1521, 1396, 485, 9235, 2096, 694, 908, 1792, 4815, 4981, 272, 38962, 1430, 8645, 590, 457, 11205, 1388, 38398, 10414, 1045, 1456, 730, 3032, 7743, 2575, 3037, 1178, 6361, 2919, 1779, 733, 1843, 765, 5017, 650, 6007, 1633, 20989, 1455, 1531, 8424, 3647, 673, 15552, 1024, 23355, 11410, 717, 2324, 1537, 27335, 13735, 353, 730, 1851, 34, 929, 1737, 150, 1905, 13869, 47154, 308, 91, 3043, 558, 5215, 47, 130, 1269, 62, 19785, 20731, 126, 999, 1744, 94, 1126, 60, 551, 14019, 3694, 86, 15026, 1502, 3132, 73, 2753, 26612, 1849, 236, 1048, 34136, 1022, 44, 15278, 529, 9428, 506, 1477, 2080, 1313, 47, 406, 3553, 428, 47, 343, 84, 40669, 588, 26891, 59, 211, 892, 7866, 7754, 13301, 819, 38, 1856, 123, 555, 49376, 156, 162, 7465, 1525, 33, 648, 17317, 1217, 30476, 4838, 81, 310, 183, 3465, 82, 805, 50, 300, 340, 275, 144, 1160, 123, 18158, 290, 1374, 181, 12778, 675, 295, 2540, 7511, 16966, 6395, 1244, 10198, 10482, 181, 407, 10132, 13140, 23666, 650, 170, 289, 4803, 123, 3566, 46589, 51, 432, 1888, 1992, 369, 1667, 1901, 6209, 4715, 324, 908, 2741, 2666, 4981, 675, 295, 2450, 35900, 2025, 6464, 1178, 6238, 1547, 1227, 11583, 841, 24, 4261, 186, 708, 650, 4902, 10994, 213, 521, 18547, 16280, 432, 1771, 1992, 21748, 26947, 2222, 1880, 717, 31, 2493, 25010, 3782, 295, 869, 7542, 2401, 5658, 3617, 807, 6889, 2923, 24832, 1483, 1468, 1771, 3690, 1654, 32011, 938, 92, 47221, 2386, 409, 1296, 86, 552, 428, 46932, 40, 1086, 51, 368, 2203, 332, 27833, 176, 1215, 964, 33, 438, 1242, 211, 212, 376, 5798, 123, 1278, 5993, 461, 13830, 6202, 280, 40, 14738, 5750, 2815, 96, 2847, 100, 36031, 929, 44553, 708, 24437, 469, 743, 44, 1187, 103, 634, 8645, 17377, 1022, 335, 93, 5071, 7179, 3882, 741, 3452, 753, 252, 300, 9848, 33060, 702, 221, 7210, 56, 2463, 3302, 49, 1319, 1168, 155, 379, 272, 2570, 824, 728, 25, 2945, 341, 21, 241, 2014, 8364, 670, 109, 9929, 13088, 536, 7618, 102, 35646, 11433, 620, 345, 330, 371, 1400, 40595, 65, 897, 980, 1737, 43001, 10112, 30152, 239, 215, 2674, 23, 750, 619, 10308, 12541, 105, 13411, 977, 45755, 2099, 40, 2278, 6535, 192, 290, 31465, 627, 8606, 21298, 186, 2517, 3350, 860, 209, 552, 2589, 1119, 11340, 1378, 27087, 3785, 17213, 17823, 2489, 1144, 892, 41814, 15691, 123, 5139, 18091, 7599, 41700, 41616, 34473, 1267, 276, 1368, 221, 9605, 521, 748, 40, 14989, 43, 224, 15035, 121, 2680, 26986, 8482, 762, 196, 207, 24045, 1539, 354, 1227, 8415, 1477, 579, 61, 215, 1890, 156, 141, 8522, 13148, 1705, 56, 495, 5474, 35943, 746, 55, 995, 731, 32559, 17005, 759, 2280, 349, 57, 2884, 173, 553, 4750, 575, 56, 42172, 516, 19511, 957, 23239, 570, 59, 560, 241, 3607, 145, 335, 10897, 1111, 29, 2547, 55, 272, 38353, 459, 1962, 10673, 305, 2447, 678, 277, 6578, 720, 18563, 2717, 16251, 144, 3211, 106, 524, 48, 372, 332, 182, 151, 1755, 643, 2646, 5701, 191, 868, 158, 846, 760, 23971, 58, 21094, 321, 1868, 10274, 3133, 262, 7496, 241, 2622, 340, 1507, 47119, 593, 809, 24562, 5556, 22621, 21384, 230, 25412, 25320, 761, 997, 2520, 18024, 219, 2918, 19438, 26, 1667, 167, 16811, 5712, 630, 1667, 1037, 3082, 359, 590, 151, 11358, 420, 1168, 4716, 2039, 316, 1199, 21040, 344, 3095, 49227, 1138, 114, 470, 5825, 3258, 13078, 1227, 26, 7412, 4716, 13405, 139, 523, 200, 979, 13378, 385, 8388, 520, 23074, 500, 379, 698, 14680, 518, 7393, 2789, 13743, 96, 224, 100, 1218, 13384, 2083, 4823, 186, 4397, 472, 20869, 597, 3228, 70, 104, 209, 199, 1252, 2744, 178, 208, 206, 438, 786, 7065, 5915, 201, 328, 295, 2365, 2769, 182, 2480, 1738, 217, 898, 6711, 8474, 587, 18686, 645, 5116, 532, 2346, 35400, 3856, 1035, 4971, 205, 124, 205, 241, 109, 1306, 6540, 13211, 907, 78, 3414, 146, 666, 29147, 70, 268, 4250, 2845, 216, 17007, 60, 2003, 4302, 127, 8529, 1004, 146, 4475, 119, 2910, 301, 73, 719, 7988, 35479, 21725, 6926, 28, 8255, 374, 1716, 160, 215, 385, 692, 4338, 8398, 3359, 517, 738, 838, 1233, 24674, 92, 1898, 33511, 613, 15744, 955, 3714, 4499, 1791, 5677, 692, 10419, 9165, 44650, 517, 738, 323, 1233, 330, 9753, 92, 33766, 451, 18230, 2413, 754, 955, 48019, 3860, 899, 952, 14685, 3016, 2385, 47207, 372, 8509, 326, 176, 1496, 856, 3707, 4277, 2068, 16716, 2694, 212, 837, 321, 32398, 80, 658, 13614, 1074, 1748, 22972, 180, 1213, 924, 349, 2228, 5838, 354, 4220, 1290, 7763, 5989, 5522, 87, 1375, 375, 515, 59, 1988, 6106, 9230, 111, 286, 366, 757, 9821, 2847, 2900, 295, 541, 15485, 100, 9422, 296, 18555, 25905, 72, 1702, 5155, 26527, 573, 15302, 467, 3090, 1201, 141, 20292, 384, 5394, 817, 2500, 26785, 1552, 2401, 8230, 5088, 7, 11538, 363, 4503, 173, 2876, 139, 1483, 1468, 30147, 7076, 21721, 691, 15351, 481, 127, 4715, 3663, 155, 27825, 1339, 36, 6904, 135, 5417, 7058, 2025, 384, 10157, 688, 1059, 30813, 1227, 221, 10038, 284, 5658, 4018, 781, 2030, 4755, 4949, 3308, 1483, 1531, 30985, 3155, 82, 31640, 13155, 1778, 694, 4757, 14103, 1691, 199, 3090, 809, 4497, 32, 1383, 203, 5168, 1994, 3201, 264, 1025, 49, 273, 37407, 1179, 3673, 341, 472, 583, 88, 2213, 180, 212, 929, 179, 4888, 46931, 20, 2987, 145, 1611, 8311, 104, 2551, 7853, 227, 1562, 844, 1698, 123, 37838, 6815, 93, 826, 1482, 1837, 6629, 717, 451, 2418, 225, 3138, 850, 18, 3797, 155, 2114, 3358, 187, 1716, 30752, 434, 1821, 15929, 2162, 3092, 144, 11478, 757, 60, 25780, 4116, 715, 131, 43591, 1061, 283, 695, 235, 555, 4005, 2942, 692, 7183, 501, 24492, 517, 2147, 284, 16248, 2587, 10976, 182, 181, 539, 28, 9533, 1067, 955, 12511, 3889, 300, 28, 939, 198, 2687, 3576, 883, 2205, 5350, 9831, 489, 337, 2084, 84, 49031, 436, 192, 11287, 112, 1955, 231, 508, 1464, 44, 463, 3099, 688, 569, 445, 5821, 45304, 7743, 2645, 4391, 1178, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
04/11/2024 22:16:45 - INFO - __main__ - ^^^^^^^^tf32 is set: True
04/11/2024 22:16:46 - INFO - __main__ - model size: 371.2M parameters
04/11/2024 22:16:46 - WARNING - accelerate.utils.other - Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:552] 2024-04-11 22:16:46,875 >> max_steps is given, it will override any value given in num_train_epochs
04/11/2024 22:16:46 - INFO - __main__ - >>>>>>>>>>>>>>>>Start training and evaluatoin......
length of sample: 1024
length of sample: 1024
length of sample: 1024
total samples: 1482
total samples: 1482
total samples: 1482
{'input_ids': [623, 3330, 67, 1916, 326, 1279, 226, 668, 104, 2306, 50, 526, 1243, 83, 1168, 30, 1294, 9375, 1723, 127, 384, 3380, 4932, 30, 3250, 169, 38503, 813, 1804, 125, 650, 2627, 1389, 360, 1483, 14420, 19426, 1521, 1396, 485, 9235, 2096, 694, 908, 1792, 4815, 4981, 272, 38962, 1430, 8645, 590, 457, 11205, 1388, 38398, 10414, 1045, 1456, 730, 3032, 7743, 2575, 3037, 1178, 6361, 2919, 1779, 733, 1843, 765, 5017, 650, 6007, 1633, 20989, 1455, 1531, 8424, 3647, 673, 15552, 1024, 23355, 11410, 717, 2324, 1537, 27335, 13735, 353, 730, 1851, 34, 929, 1737, 150, 1905, 13869, 47154, 308, 91, 3043, 558, 5215, 47, 130, 1269, 62, 19785, 20731, 126, 999, 1744, 94, 1126, 60, 551, 14019, 3694, 86, 15026, 1502, 3132, 73, 2753, 26612, 1849, 236, 1048, 34136, 1022, 44, 15278, 529, 9428, 506, 1477, 2080, 1313, 47, 406, 3553, 428, 47, 343, 84, 40669, 588, 26891, 59, 211, 892, 7866, 7754, 13301, 819, 38, 1856, 123, 555, 49376, 156, 162, 7465, 1525, 33, 648, 17317, 1217, 30476, 4838, 81, 310, 183, 3465, 82, 805, 50, 300, 340, 275, 144, 1160, 123, 18158, 290, 1374, 181, 12778, 675, 295, 2540, 7511, 16966, 6395, 1244, 10198, 10482, 181, 407, 10132, 13140, 23666, 650, 170, 289, 4803, 123, 3566, 46589, 51, 432, 1888, 1992, 369, 1667, 1901, 6209, 4715, 324, 908, 2741, 2666, 4981, 675, 295, 2450, 35900, 2025, 6464, 1178, 6238, 1547, 1227, 11583, 841, 24, 4261, 186, 708, 650, 4902, 10994, 213, 521, 18547, 16280, 432, 1771, 1992, 21748, 26947, 2222, 1880, 717, 31, 2493, 25010, 3782, 295, 869, 7542, 2401, 5658, 3617, 807, 6889, 2923, 24832, 1483, 1468, 1771, 3690, 1654, 32011, 938, 92, 47221, 2386, 409, 1296, 86, 552, 428, 46932, 40, 1086, 51, 368, 2203, 332, 27833, 176, 1215, 964, 33, 438, 1242, 211, 212, 376, 5798, 123, 1278, 5993, 461, 13830, 6202, 280, 40, 14738, 5750, 2815, 96, 2847, 100, 36031, 929, 44553, 708, 24437, 469, 743, 44, 1187, 103, 634, 8645, 17377, 1022, 335, 93, 5071, 7179, 3882, 741, 3452, 753, 252, 300, 9848, 33060, 702, 221, 7210, 56, 2463, 3302, 49, 1319, 1168, 155, 379, 272, 2570, 824, 728, 25, 2945, 341, 21, 241, 2014, 8364, 670, 109, 9929, 13088, 536, 7618, 102, 35646, 11433, 620, 345, 330, 371, 1400, 40595, 65, 897, 980, 1737, 43001, 10112, 30152, 239, 215, 2674, 23, 750, 619, 10308, 12541, 105, 13411, 977, 45755, 2099, 40, 2278, 6535, 192, 290, 31465, 627, 8606, 21298, 186, 2517, 3350, 860, 209, 552, 2589, 1119, 11340, 1378, 27087, 3785, 17213, 17823, 2489, 1144, 892, 41814, 15691, 123, 5139, 18091, 7599, 41700, 41616, 34473, 1267, 276, 1368, 221, 9605, 521, 748, 40, 14989, 43, 224, 15035, 121, 2680, 26986, 8482, 762, 196, 207, 24045, 1539, 354, 1227, 8415, 1477, 579, 61, 215, 1890, 156, 141, 8522, 13148, 1705, 56, 495, 5474, 35943, 746, 55, 995, 731, 32559, 17005, 759, 2280, 349, 57, 2884, 173, 553, 4750, 575, 56, 42172, 516, 19511, 957, 23239, 570, 59, 560, 241, 3607, 145, 335, 10897, 1111, 29, 2547, 55, 272, 38353, 459, 1962, 10673, 305, 2447, 678, 277, 6578, 720, 18563, 2717, 16251, 144, 3211, 106, 524, 48, 372, 332, 182, 151, 1755, 643, 2646, 5701, 191, 868, 158, 846, 760, 23971, 58, 21094, 321, 1868, 10274, 3133, 262, 7496, 241, 2622, 340, 1507, 47119, 593, 809, 24562, 5556, 22621, 21384, 230, 25412, 25320, 761, 997, 2520, 18024, 219, 2918, 19438, 26, 1667, 167, 16811, 5712, 630, 1667, 1037, 3082, 359, 590, 151, 11358, 420, 1168, 4716, 2039, 316, 1199, 21040, 344, 3095, 49227, 1138, 114, 470, 5825, 3258, 13078, 1227, 26, 7412, 4716, 13405, 139, 523, 200, 979, 13378, 385, 8388, 520, 23074, 500, 379, 698, 14680, 518, 7393, 2789, 13743, 96, 224, 100, 1218, 13384, 2083, 4823, 186, 4397, 472, 20869, 597, 3228, 70, 104, 209, 199, 1252, 2744, 178, 208, 206, 438, 786, 7065, 5915, 201, 328, 295, 2365, 2769, 182, 2480, 1738, 217, 898, 6711, 8474, 587, 18686, 645, 5116, 532, 2346, 35400, 3856, 1035, 4971, 205, 124, 205, 241, 109, 1306, 6540, 13211, 907, 78, 3414, 146, 666, 29147, 70, 268, 4250, 2845, 216, 17007, 60, 2003, 4302, 127, 8529, 1004, 146, 4475, 119, 2910, 301, 73, 719, 7988, 35479, 21725, 6926, 28, 8255, 374, 1716, 160, 215, 385, 692, 4338, 8398, 3359, 517, 738, 838, 1233, 24674, 92, 1898, 33511, 613, 15744, 955, 3714, 4499, 1791, 5677, 692, 10419, 9165, 44650, 517, 738, 323, 1233, 330, 9753, 92, 33766, 451, 18230, 2413, 754, 955, 48019, 3860, 899, 952, 14685, 3016, 2385, 47207, 372, 8509, 326, 176, 1496, 856, 3707, 4277, 2068, 16716, 2694, 212, 837, 321, 32398, 80, 658, 13614, 1074, 1748, 22972, 180, 1213, 924, 349, 2228, 5838, 354, 4220, 1290, 7763, 5989, 5522, 87, 1375, 375, 515, 59, 1988, 6106, 9230, 111, 286, 366, 757, 9821, 2847, 2900, 295, 541, 15485, 100, 9422, 296, 18555, 25905, 72, 1702, 5155, 26527, 573, 15302, 467, 3090, 1201, 141, 20292, 384, 5394, 817, 2500, 26785, 1552, 2401, 8230, 5088, 7, 11538, 363, 4503, 173, 2876, 139, 1483, 1468, 30147, 7076, 21721, 691, 15351, 481, 127, 4715, 3663, 155, 27825, 1339, 36, 6904, 135, 5417, 7058, 2025, 384, 10157, 688, 1059, 30813, 1227, 221, 10038, 284, 5658, 4018, 781, 2030, 4755, 4949, 3308, 1483, 1531, 30985, 3155, 82, 31640, 13155, 1778, 694, 4757, 14103, 1691, 199, 3090, 809, 4497, 32, 1383, 203, 5168, 1994, 3201, 264, 1025, 49, 273, 37407, 1179, 3673, 341, 472, 583, 88, 2213, 180, 212, 929, 179, 4888, 46931, 20, 2987, 145, 1611, 8311, 104, 2551, 7853, 227, 1562, 844, 1698, 123, 37838, 6815, 93, 826, 1482, 1837, 6629, 717, 451, 2418, 225, 3138, 850, 18, 3797, 155, 2114, 3358, 187, 1716, 30752, 434, 1821, 15929, 2162, 3092, 144, 11478, 757, 60, 25780, 4116, 715, 131, 43591, 1061, 283, 695, 235, 555, 4005, 2942, 692, 7183, 501, 24492, 517, 2147, 284, 16248, 2587, 10976, 182, 181, 539, 28, 9533, 1067, 955, 12511, 3889, 300, 28, 939, 198, 2687, 3576, 883, 2205, 5350, 9831, 489, 337, 2084, 84, 49031, 436, 192, 11287, 112, 1955, 231, 508, 1464, 44, 463, 3099, 688, 569, 445, 5821, 45304, 7743, 2645, 4391, 1178, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
{'input_ids': [623, 3330, 67, 1916, 326, 1279, 226, 668, 104, 2306, 50, 526, 1243, 83, 1168, 30, 1294, 9375, 1723, 127, 384, 3380, 4932, 30, 3250, 169, 38503, 813, 1804, 125, 650, 2627, 1389, 360, 1483, 14420, 19426, 1521, 1396, 485, 9235, 2096, 694, 908, 1792, 4815, 4981, 272, 38962, 1430, 8645, 590, 457, 11205, 1388, 38398, 10414, 1045, 1456, 730, 3032, 7743, 2575, 3037, 1178, 6361, 2919, 1779, 733, 1843, 765, 5017, 650, 6007, 1633, 20989, 1455, 1531, 8424, 3647, 673, 15552, 1024, 23355, 11410, 717, 2324, 1537, 27335, 13735, 353, 730, 1851, 34, 929, 1737, 150, 1905, 13869, 47154, 308, 91, 3043, 558, 5215, 47, 130, 1269, 62, 19785, 20731, 126, 999, 1744, 94, 1126, 60, 551, 14019, 3694, 86, 15026, 1502, 3132, 73, 2753, 26612, 1849, 236, 1048, 34136, 1022, 44, 15278, 529, 9428, 506, 1477, 2080, 1313, 47, 406, 3553, 428, 47, 343, 84, 40669, 588, 26891, 59, 211, 892, 7866, 7754, 13301, 819, 38, 1856, 123, 555, 49376, 156, 162, 7465, 1525, 33, 648, 17317, 1217, 30476, 4838, 81, 310, 183, 3465, 82, 805, 50, 300, 340, 275, 144, 1160, 123, 18158, 290, 1374, 181, 12778, 675, 295, 2540, 7511, 16966, 6395, 1244, 10198, 10482, 181, 407, 10132, 13140, 23666, 650, 170, 289, 4803, 123, 3566, 46589, 51, 432, 1888, 1992, 369, 1667, 1901, 6209, 4715, 324, 908, 2741, 2666, 4981, 675, 295, 2450, 35900, 2025, 6464, 1178, 6238, 1547, 1227, 11583, 841, 24, 4261, 186, 708, 650, 4902, 10994, 213, 521, 18547, 16280, 432, 1771, 1992, 21748, 26947, 2222, 1880, 717, 31, 2493, 25010, 3782, 295, 869, 7542, 2401, 5658, 3617, 807, 6889, 2923, 24832, 1483, 1468, 1771, 3690, 1654, 32011, 938, 92, 47221, 2386, 409, 1296, 86, 552, 428, 46932, 40, 1086, 51, 368, 2203, 332, 27833, 176, 1215, 964, 33, 438, 1242, 211, 212, 376, 5798, 123, 1278, 5993, 461, 13830, 6202, 280, 40, 14738, 5750, 2815, 96, 2847, 100, 36031, 929, 44553, 708, 24437, 469, 743, 44, 1187, 103, 634, 8645, 17377, 1022, 335, 93, 5071, 7179, 3882, 741, 3452, 753, 252, 300, 9848, 33060, 702, 221, 7210, 56, 2463, 3302, 49, 1319, 1168, 155, 379, 272, 2570, 824, 728, 25, 2945, 341, 21, 241, 2014, 8364, 670, 109, 9929, 13088, 536, 7618, 102, 35646, 11433, 620, 345, 330, 371, 1400, 40595, 65, 897, 980, 1737, 43001, 10112, 30152, 239, 215, 2674, 23, 750, 619, 10308, 12541, 105, 13411, 977, 45755, 2099, 40, 2278, 6535, 192, 290, 31465, 627, 8606, 21298, 186, 2517, 3350, 860, 209, 552, 2589, 1119, 11340, 1378, 27087, 3785, 17213, 17823, 2489, 1144, 892, 41814, 15691, 123, 5139, 18091, 7599, 41700, 41616, 34473, 1267, 276, 1368, 221, 9605, 521, 748, 40, 14989, 43, 224, 15035, 121, 2680, 26986, 8482, 762, 196, 207, 24045, 1539, 354, 1227, 8415, 1477, 579, 61, 215, 1890, 156, 141, 8522, 13148, 1705, 56, 495, 5474, 35943, 746, 55, 995, 731, 32559, 17005, 759, 2280, 349, 57, 2884, 173, 553, 4750, 575, 56, 42172, 516, 19511, 957, 23239, 570, 59, 560, 241, 3607, 145, 335, 10897, 1111, 29, 2547, 55, 272, 38353, 459, 1962, 10673, 305, 2447, 678, 277, 6578, 720, 18563, 2717, 16251, 144, 3211, 106, 524, 48, 372, 332, 182, 151, 1755, 643, 2646, 5701, 191, 868, 158, 846, 760, 23971, 58, 21094, 321, 1868, 10274, 3133, 262, 7496, 241, 2622, 340, 1507, 47119, 593, 809, 24562, 5556, 22621, 21384, 230, 25412, 25320, 761, 997, 2520, 18024, 219, 2918, 19438, 26, 1667, 167, 16811, 5712, 630, 1667, 1037, 3082, 359, 590, 151, 11358, 420, 1168, 4716, 2039, 316, 1199, 21040, 344, 3095, 49227, 1138, 114, 470, 5825, 3258, 13078, 1227, 26, 7412, 4716, 13405, 139, 523, 200, 979, 13378, 385, 8388, 520, 23074, 500, 379, 698, 14680, 518, 7393, 2789, 13743, 96, 224, 100, 1218, 13384, 2083, 4823, 186, 4397, 472, 20869, 597, 3228, 70, 104, 209, 199, 1252, 2744, 178, 208, 206, 438, 786, 7065, 5915, 201, 328, 295, 2365, 2769, 182, 2480, 1738, 217, 898, 6711, 8474, 587, 18686, 645, 5116, 532, 2346, 35400, 3856, 1035, 4971, 205, 124, 205, 241, 109, 1306, 6540, 13211, 907, 78, 3414, 146, 666, 29147, 70, 268, 4250, 2845, 216, 17007, 60, 2003, 4302, 127, 8529, 1004, 146, 4475, 119, 2910, 301, 73, 719, 7988, 35479, 21725, 6926, 28, 8255, 374, 1716, 160, 215, 385, 692, 4338, 8398, 3359, 517, 738, 838, 1233, 24674, 92, 1898, 33511, 613, 15744, 955, 3714, 4499, 1791, 5677, 692, 10419, 9165, 44650, 517, 738, 323, 1233, 330, 9753, 92, 33766, 451, 18230, 2413, 754, 955, 48019, 3860, 899, 952, 14685, 3016, 2385, 47207, 372, 8509, 326, 176, 1496, 856, 3707, 4277, 2068, 16716, 2694, 212, 837, 321, 32398, 80, 658, 13614, 1074, 1748, 22972, 180, 1213, 924, 349, 2228, 5838, 354, 4220, 1290, 7763, 5989, 5522, 87, 1375, 375, 515, 59, 1988, 6106, 9230, 111, 286, 366, 757, 9821, 2847, 2900, 295, 541, 15485, 100, 9422, 296, 18555, 25905, 72, 1702, 5155, 26527, 573, 15302, 467, 3090, 1201, 141, 20292, 384, 5394, 817, 2500, 26785, 1552, 2401, 8230, 5088, 7, 11538, 363, 4503, 173, 2876, 139, 1483, 1468, 30147, 7076, 21721, 691, 15351, 481, 127, 4715, 3663, 155, 27825, 1339, 36, 6904, 135, 5417, 7058, 2025, 384, 10157, 688, 1059, 30813, 1227, 221, 10038, 284, 5658, 4018, 781, 2030, 4755, 4949, 3308, 1483, 1531, 30985, 3155, 82, 31640, 13155, 1778, 694, 4757, 14103, 1691, 199, 3090, 809, 4497, 32, 1383, 203, 5168, 1994, 3201, 264, 1025, 49, 273, 37407, 1179, 3673, 341, 472, 583, 88, 2213, 180, 212, 929, 179, 4888, 46931, 20, 2987, 145, 1611, 8311, 104, 2551, 7853, 227, 1562, 844, 1698, 123, 37838, 6815, 93, 826, 1482, 1837, 6629, 717, 451, 2418, 225, 3138, 850, 18, 3797, 155, 2114, 3358, 187, 1716, 30752, 434, 1821, 15929, 2162, 3092, 144, 11478, 757, 60, 25780, 4116, 715, 131, 43591, 1061, 283, 695, 235, 555, 4005, 2942, 692, 7183, 501, 24492, 517, 2147, 284, 16248, 2587, 10976, 182, 181, 539, 28, 9533, 1067, 955, 12511, 3889, 300, 28, 939, 198, 2687, 3576, 883, 2205, 5350, 9831, 489, 337, 2084, 84, 49031, 436, 192, 11287, 112, 1955, 231, 508, 1464, 44, 463, 3099, 688, 569, 445, 5821, 45304, 7743, 2645, 4391, 1178, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
{'input_ids': [623, 3330, 67, 1916, 326, 1279, 226, 668, 104, 2306, 50, 526, 1243, 83, 1168, 30, 1294, 9375, 1723, 127, 384, 3380, 4932, 30, 3250, 169, 38503, 813, 1804, 125, 650, 2627, 1389, 360, 1483, 14420, 19426, 1521, 1396, 485, 9235, 2096, 694, 908, 1792, 4815, 4981, 272, 38962, 1430, 8645, 590, 457, 11205, 1388, 38398, 10414, 1045, 1456, 730, 3032, 7743, 2575, 3037, 1178, 6361, 2919, 1779, 733, 1843, 765, 5017, 650, 6007, 1633, 20989, 1455, 1531, 8424, 3647, 673, 15552, 1024, 23355, 11410, 717, 2324, 1537, 27335, 13735, 353, 730, 1851, 34, 929, 1737, 150, 1905, 13869, 47154, 308, 91, 3043, 558, 5215, 47, 130, 1269, 62, 19785, 20731, 126, 999, 1744, 94, 1126, 60, 551, 14019, 3694, 86, 15026, 1502, 3132, 73, 2753, 26612, 1849, 236, 1048, 34136, 1022, 44, 15278, 529, 9428, 506, 1477, 2080, 1313, 47, 406, 3553, 428, 47, 343, 84, 40669, 588, 26891, 59, 211, 892, 7866, 7754, 13301, 819, 38, 1856, 123, 555, 49376, 156, 162, 7465, 1525, 33, 648, 17317, 1217, 30476, 4838, 81, 310, 183, 3465, 82, 805, 50, 300, 340, 275, 144, 1160, 123, 18158, 290, 1374, 181, 12778, 675, 295, 2540, 7511, 16966, 6395, 1244, 10198, 10482, 181, 407, 10132, 13140, 23666, 650, 170, 289, 4803, 123, 3566, 46589, 51, 432, 1888, 1992, 369, 1667, 1901, 6209, 4715, 324, 908, 2741, 2666, 4981, 675, 295, 2450, 35900, 2025, 6464, 1178, 6238, 1547, 1227, 11583, 841, 24, 4261, 186, 708, 650, 4902, 10994, 213, 521, 18547, 16280, 432, 1771, 1992, 21748, 26947, 2222, 1880, 717, 31, 2493, 25010, 3782, 295, 869, 7542, 2401, 5658, 3617, 807, 6889, 2923, 24832, 1483, 1468, 1771, 3690, 1654, 32011, 938, 92, 47221, 2386, 409, 1296, 86, 552, 428, 46932, 40, 1086, 51, 368, 2203, 332, 27833, 176, 1215, 964, 33, 438, 1242, 211, 212, 376, 5798, 123, 1278, 5993, 461, 13830, 6202, 280, 40, 14738, 5750, 2815, 96, 2847, 100, 36031, 929, 44553, 708, 24437, 469, 743, 44, 1187, 103, 634, 8645, 17377, 1022, 335, 93, 5071, 7179, 3882, 741, 3452, 753, 252, 300, 9848, 33060, 702, 221, 7210, 56, 2463, 3302, 49, 1319, 1168, 155, 379, 272, 2570, 824, 728, 25, 2945, 341, 21, 241, 2014, 8364, 670, 109, 9929, 13088, 536, 7618, 102, 35646, 11433, 620, 345, 330, 371, 1400, 40595, 65, 897, 980, 1737, 43001, 10112, 30152, 239, 215, 2674, 23, 750, 619, 10308, 12541, 105, 13411, 977, 45755, 2099, 40, 2278, 6535, 192, 290, 31465, 627, 8606, 21298, 186, 2517, 3350, 860, 209, 552, 2589, 1119, 11340, 1378, 27087, 3785, 17213, 17823, 2489, 1144, 892, 41814, 15691, 123, 5139, 18091, 7599, 41700, 41616, 34473, 1267, 276, 1368, 221, 9605, 521, 748, 40, 14989, 43, 224, 15035, 121, 2680, 26986, 8482, 762, 196, 207, 24045, 1539, 354, 1227, 8415, 1477, 579, 61, 215, 1890, 156, 141, 8522, 13148, 1705, 56, 495, 5474, 35943, 746, 55, 995, 731, 32559, 17005, 759, 2280, 349, 57, 2884, 173, 553, 4750, 575, 56, 42172, 516, 19511, 957, 23239, 570, 59, 560, 241, 3607, 145, 335, 10897, 1111, 29, 2547, 55, 272, 38353, 459, 1962, 10673, 305, 2447, 678, 277, 6578, 720, 18563, 2717, 16251, 144, 3211, 106, 524, 48, 372, 332, 182, 151, 1755, 643, 2646, 5701, 191, 868, 158, 846, 760, 23971, 58, 21094, 321, 1868, 10274, 3133, 262, 7496, 241, 2622, 340, 1507, 47119, 593, 809, 24562, 5556, 22621, 21384, 230, 25412, 25320, 761, 997, 2520, 18024, 219, 2918, 19438, 26, 1667, 167, 16811, 5712, 630, 1667, 1037, 3082, 359, 590, 151, 11358, 420, 1168, 4716, 2039, 316, 1199, 21040, 344, 3095, 49227, 1138, 114, 470, 5825, 3258, 13078, 1227, 26, 7412, 4716, 13405, 139, 523, 200, 979, 13378, 385, 8388, 520, 23074, 500, 379, 698, 14680, 518, 7393, 2789, 13743, 96, 224, 100, 1218, 13384, 2083, 4823, 186, 4397, 472, 20869, 597, 3228, 70, 104, 209, 199, 1252, 2744, 178, 208, 206, 438, 786, 7065, 5915, 201, 328, 295, 2365, 2769, 182, 2480, 1738, 217, 898, 6711, 8474, 587, 18686, 645, 5116, 532, 2346, 35400, 3856, 1035, 4971, 205, 124, 205, 241, 109, 1306, 6540, 13211, 907, 78, 3414, 146, 666, 29147, 70, 268, 4250, 2845, 216, 17007, 60, 2003, 4302, 127, 8529, 1004, 146, 4475, 119, 2910, 301, 73, 719, 7988, 35479, 21725, 6926, 28, 8255, 374, 1716, 160, 215, 385, 692, 4338, 8398, 3359, 517, 738, 838, 1233, 24674, 92, 1898, 33511, 613, 15744, 955, 3714, 4499, 1791, 5677, 692, 10419, 9165, 44650, 517, 738, 323, 1233, 330, 9753, 92, 33766, 451, 18230, 2413, 754, 955, 48019, 3860, 899, 952, 14685, 3016, 2385, 47207, 372, 8509, 326, 176, 1496, 856, 3707, 4277, 2068, 16716, 2694, 212, 837, 321, 32398, 80, 658, 13614, 1074, 1748, 22972, 180, 1213, 924, 349, 2228, 5838, 354, 4220, 1290, 7763, 5989, 5522, 87, 1375, 375, 515, 59, 1988, 6106, 9230, 111, 286, 366, 757, 9821, 2847, 2900, 295, 541, 15485, 100, 9422, 296, 18555, 25905, 72, 1702, 5155, 26527, 573, 15302, 467, 3090, 1201, 141, 20292, 384, 5394, 817, 2500, 26785, 1552, 2401, 8230, 5088, 7, 11538, 363, 4503, 173, 2876, 139, 1483, 1468, 30147, 7076, 21721, 691, 15351, 481, 127, 4715, 3663, 155, 27825, 1339, 36, 6904, 135, 5417, 7058, 2025, 384, 10157, 688, 1059, 30813, 1227, 221, 10038, 284, 5658, 4018, 781, 2030, 4755, 4949, 3308, 1483, 1531, 30985, 3155, 82, 31640, 13155, 1778, 694, 4757, 14103, 1691, 199, 3090, 809, 4497, 32, 1383, 203, 5168, 1994, 3201, 264, 1025, 49, 273, 37407, 1179, 3673, 341, 472, 583, 88, 2213, 180, 212, 929, 179, 4888, 46931, 20, 2987, 145, 1611, 8311, 104, 2551, 7853, 227, 1562, 844, 1698, 123, 37838, 6815, 93, 826, 1482, 1837, 6629, 717, 451, 2418, 225, 3138, 850, 18, 3797, 155, 2114, 3358, 187, 1716, 30752, 434, 1821, 15929, 2162, 3092, 144, 11478, 757, 60, 25780, 4116, 715, 131, 43591, 1061, 283, 695, 235, 555, 4005, 2942, 692, 7183, 501, 24492, 517, 2147, 284, 16248, 2587, 10976, 182, 181, 539, 28, 9533, 1067, 955, 12511, 3889, 300, 28, 939, 198, 2687, 3576, 883, 2205, 5350, 9831, 489, 337, 2084, 84, 49031, 436, 192, 11287, 112, 1955, 231, 508, 1464, 44, 463, 3099, 688, 569, 445, 5821, 45304, 7743, 2645, 4391, 1178, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
[INFO|trainer.py:1812] 2024-04-11 22:17:02,487 >> ***** Running training *****
[INFO|trainer.py:1813] 2024-04-11 22:17:02,487 >>   Num examples = 12,800
[INFO|trainer.py:1814] 2024-04-11 22:17:02,487 >>   Num Epochs = 9,223,372,036,854,775,807
[INFO|trainer.py:1815] 2024-04-11 22:17:02,487 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1818] 2024-04-11 22:17:02,487 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1819] 2024-04-11 22:17:02,488 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1820] 2024-04-11 22:17:02,488 >>   Total optimization steps = 800
[INFO|trainer.py:1821] 2024-04-11 22:17:02,489 >>   Number of trainable parameters = 371,246,080
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 369, in reduce_storage
    fd, size = storage._share_fd_cpu_()
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 369, in reduce_storage
    fd, size = storage._share_fd_cpu_()
RuntimeError: unable to write to file </torch_3837705_3719323563_0>: No space left on device (28)
RuntimeError: unable to write to file </torch_3837835_2308454389_0>: No space left on device (28)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 369, in reduce_storage
    fd, size = storage._share_fd_cpu_()
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 369, in reduce_storage
    fd, size = storage._share_fd_cpu_()
RuntimeError: unable to write to file </torch_3837705_256293503_1>: No space left on device (28)
RuntimeError: unable to write to file </torch_3837835_1649162644_1>: No space left on device (28)
Traceback (most recent call last):
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 369, in reduce_storage
    fd, size = storage._share_fd_cpu_()
RuntimeError: unable to write to file </torch_3837909_3761813682_0>: No space left on device (28)
Traceback (most recent call last):
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 369, in reduce_storage
    fd, size = storage._share_fd_cpu_()
RuntimeError: unable to write to file </torch_3837909_424429980_1>: No space left on device (28)
Traceback (most recent call last):
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 369, in reduce_storage
    fd, size = storage._share_fd_cpu_()
RuntimeError: unable to write to file </torch_3837768_4060794091_0>: No space left on device (28)
Traceback (most recent call last):
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/HPCBase/tools/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 369, in reduce_storage
    fd, size = storage._share_fd_cpu_()
RuntimeError: unable to write to file </torch_3837768_603952180_1>: No space left on device (28)
[E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=10, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1806127 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=10, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1806128 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=10, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1806127 milliseconds before timing out.
Traceback (most recent call last):
  File "/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/train/run_bioseqmamba_causal.py", line 566, in <module>
    main()
  File "/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/train/run_bioseqmamba_causal.py", line 531, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/transformers/trainer.py", line 1928, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/data_loader.py", line 654, in __iter__
    Traceback (most recent call last):
next_batch, next_batch_info = self._fetch_batches(main_iterator)  File "/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/train/run_bioseqmamba_causal.py", line 566, in <module>

  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/data_loader.py", line 627, in _fetch_batches
    broadcast_object_list(batch_info)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/utils/operations.py", line 491, in broadcast_object_list
    main()
  File "/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/train/run_bioseqmamba_causal.py", line 531, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train
    torch.distributed.broadcast_object_list(object_list, src=from_process)
      File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1436, in wrapper
return inner_training_loop(
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/transformers/trainer.py", line 1928, in _inner_training_loop
Traceback (most recent call last):
  File "/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/train/run_bioseqmamba_causal.py", line 566, in <module>
    for step, inputs in enumerate(epoch_iterator):
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/data_loader.py", line 654, in __iter__
    main()
  File "/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/train/run_bioseqmamba_causal.py", line 531, in main
    next_batch, next_batch_info = self._fetch_batches(main_iterator)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/data_loader.py", line 627, in _fetch_batches
    train_result = trainer.train(resume_from_checkpoint=checkpoint)    
broadcast_object_list(batch_info)  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train

  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/utils/operations.py", line 491, in broadcast_object_list
    torch.distributed.broadcast_object_list(object_list, src=from_process)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1436, in wrapper
    return inner_training_loop(
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/transformers/trainer.py", line 1928, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/data_loader.py", line 654, in __iter__
    next_batch, next_batch_info = self._fetch_batches(main_iterator)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/data_loader.py", line 627, in _fetch_batches
    broadcast_object_list(batch_info)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/accelerate/utils/operations.py", line 491, in broadcast_object_list
    torch.distributed.broadcast_object_list(object_list, src=from_process)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1436, in wrapper
            return func(*args, **kwargs)return func(*args, **kwargs)return func(*args, **kwargs)


  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2252, in broadcast_object_list
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2252, in broadcast_object_list
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2252, in broadcast_object_list
            broadcast(object_tensor, src=src, group=group)broadcast(object_tensor, src=src, group=group)broadcast(object_tensor, src=src, group=group)


  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1436, in wrapper
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1436, in wrapper
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1436, in wrapper
        return func(*args, **kwargs)return func(*args, **kwargs)    

return func(*args, **kwargs)  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1551, in broadcast
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1551, in broadcast

  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1551, in broadcast
    work = default_pg.broadcast([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=10, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1806128 milliseconds before timing out.
        work = default_pg.broadcast([tensor], opts)work = default_pg.broadcast([tensor], opts)

RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=10, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1806127 milliseconds before timing out.
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=10, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1806127 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3809575 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 3809576) of binary: /home/HPCBase/tools/anaconda3/bin/python
Traceback (most recent call last):
  File "/home/share/huadjyin/home/baiyong01/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/share/huadjyin/home/baiyong01/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
run_bioseqmamba_causal.py FAILED
--------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-11_22:47:12
  host      : cyclone001-agent-56
  rank      : 2 (local_rank: 2)
  exitcode  : -6 (pid: 3809577)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3809577
[2]:
  time      : 2024-04-11_22:47:12
  host      : cyclone001-agent-56
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 3809578)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3809578
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-11_22:47:12
  host      : cyclone001-agent-56
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 3809576)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3809576
========================================================
